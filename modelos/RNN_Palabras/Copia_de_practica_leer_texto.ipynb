{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de practica leer texto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kR8C8Ys71k7ovPq6LTENg9YGORda8qyK",
      "authorship_tag": "ABX9TyO67/vD20D9RdNDZgKDkYe7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisFerRosas/IA_2/blob/master/modelos/RNN_Palabras/Copia_de_practica_leer_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAqI2apA4lCd",
        "colab_type": "code",
        "outputId": "d3be3d33-7728-48ba-9afd-babd07782596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/Colab Notebooks/dataset_IA2/textos"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks/dataset_IA2/textos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYVGEBQy4xAi",
        "colab_type": "code",
        "outputId": "4574a331-9af2-4779-e0f6-a8126d9ce226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Constitucion_Bolivia.pdf  cronicas_marianela.txt\n",
            "Constitucion_Bolivia.txt  training_checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9D7BacL2L9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcsNwUzCA3tH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "TAKE_SIZE = 5000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ixvVoeE0CtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oow1L-iO4nm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text1 = open('cronicas_marianela.txt',mode='r',encoding='utf-8-sig').read()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD4dvCmvLbcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "historia=text1.rsplit()#separamos en texto en palabras separadas por espacio\n",
        "#historia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlQxtHChFDpz",
        "colab_type": "code",
        "outputId": "bf2f518b-c20d-426f-8b4b-c6effb4e960b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab=np.unique(historia)# se crea el vocabulario sacando las palabras unicas\n",
        "print(len(vocab))\n",
        "vocab_size=len(vocab)# el tamanio del vocabulario"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLWO4kPNE6cR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indxPalabra = {u:i for i, u in enumerate(vocab)}# creamos un diccionario con las palabras como key y numeros como valor\n",
        "palabras = np.array(vocab)# se crea un array con las palabras\n",
        "\n",
        "text_as_int = np.array([indxPalabra[c] for c in historia])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4DvljmhLaqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed2a9808-5d7a-457f-a131-22843562d356"
      },
      "source": [
        "text_as_int# los valores de las palabras"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 402,  504,  889, ..., 7611, 1604, 1605])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh9c9VThT6Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(indxPalabra)\n",
        "#print(palabras)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "372ALGXZLZpV",
        "colab_type": "code",
        "outputId": "63af63e9-3c8f-45d9-8c59-f4bd23c5ba00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(historia))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49jTnfueL4vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#se crea lotes de 100 palabras para entrenar \n",
        "palabras_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "seq_length = 100 \n",
        "sequences = palabras_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHt68qL992O_",
        "colab_type": "code",
        "outputId": "a029bfc5-814a-4a28-e945-d336437d64b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for i in palabras_dataset.take(5):\n",
        "  print(palabras[i.numpy()])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CRÓNICAS\n",
            "DE\n",
            "MARIANELA\n",
            "1917.\n",
            "INDICE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE8Q8TuuOty4",
        "colab_type": "code",
        "outputId": "b6da1997-26c4-4583-a99e-025e6c116883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#una vista de 5 lotes de 100\n",
        "for item in sequences.take(5):\n",
        "  print(repr('\\\\'.join(palabras[item.numpy()])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'CRÓNICAS\\\\DE\\\\MARIANELA\\\\1917.\\\\INDICE\\\\Pag.\\\\Presentación\\\\en\\\\Sociedad\\\\5\\\\El\\\\matrimonio\\\\7\\\\El\\\\amor\\\\y\\\\su\\\\apariencia\\\\15\\\\El\\\\nó\\\\de\\\\las\\\\niñas\\\\18\\\\El\\\\Gancho\\\\23\\\\Las\\\\«Planchadoras»\\\\29\\\\La\\\\moda\\\\y\\\\el\\\\diablo\\\\33\\\\Los\\\\«Tramitadores»\\\\39\\\\Los\\\\afeites\\\\45\\\\Las\\\\paces\\\\51\\\\Crotalogia\\\\57\\\\Rosalía\\\\en\\\\«Los\\\\Carpinchos»\\\\63\\\\El\\\\arte\\\\de\\\\estar\\\\enferma\\\\70\\\\Las\\\\inquietudes\\\\de\\\\Petrona\\\\75\\\\Pequeña\\\\defensa\\\\de\\\\la\\\\murmuración\\\\81\\\\Los\\\\secretos\\\\84\\\\La\\\\desventura\\\\de\\\\Luisa\\\\89\\\\Desavenencia\\\\trascendental\\\\93\\\\Las\\\\reinas\\\\en\\\\la\\\\guerra\\\\98\\\\Frivolidad\\\\y\\\\tilinguismo\\\\100\\\\Inés\\\\y\\\\los\\\\cipreses\\\\110\\\\La\\\\fiesta\\\\hípica\\\\115\\\\Las'\n",
            "'angustias\\\\de\\\\mi\\\\protegida\\\\120\\\\La\\\\inutilidad\\\\de\\\\San\\\\Juan\\\\Bautista\\\\126\\\\Sin\\\\presidenta\\\\132\\\\La\\\\abuela\\\\del\\\\rey\\\\de\\\\los\\\\cipreses,\\\\o\\\\el\\\\orgullo\\\\ancestral\\\\140\\\\¡¡Desahuciado!!\\\\148\\\\La\\\\viuda\\\\de\\\\Esquilón\\\\va\\\\a\\\\Mar\\\\del\\\\Plata\\\\154\\\\ADVERTENCIA.\\\\El\\\\interés\\\\que\\\\han\\\\despertado\\\\las\\\\amenas\\\\crónicas\\\\de\\\\\"Marianela\"\\\\publicadas\\\\en\\\\la\\\\página\\\\femenina\\\\de\\\\\"LA\\\\PRENSA\"\\\\me\\\\ha\\\\inducido\\\\a\\\\solicitar\\\\del\\\\Director\\\\del\\\\gran\\\\diario,\\\\Don\\\\Ezequiel\\\\P.\\\\Paz,\\\\el\\\\permiso\\\\para\\\\editarlas.\\\\La\\\\benevolencia\\\\gentil\\\\del\\\\señor\\\\Paz\\\\ha\\\\otorgado\\\\el\\\\consentimiento,\\\\y\\\\hoy\\\\aparecen\\\\los\\\\chispeantes\\\\artículos\\\\de\\\\la\\\\distinguida\\\\escritora\\\\compilados\\\\en\\\\este\\\\elegante\\\\volumen.'\n",
            "'Notorio\\\\es\\\\el\\\\éxito\\\\creciente\\\\que\\\\han\\\\logrado\\\\estas\\\\crónicas;\\\\aparte\\\\su\\\\mérito\\\\literario,\\\\puesto\\\\de\\\\relieve\\\\en\\\\un\\\\estilo\\\\fácil,\\\\terso\\\\y\\\\armonioso,\\\\contienen\\\\otra\\\\cualidad\\\\más\\\\esencial\\\\aun,\\\\consistente\\\\en\\\\su\\\\sana\\\\orientación\\\\ética,\\\\en\\\\una\\\\crítica,\\\\suavemente\\\\irónica,\\\\de\\\\nuestros\\\\hábitos\\\\y\\\\costumbres.\\\\Trátase,\\\\en\\\\fin,\\\\de\\\\un\\\\libro\\\\interesante,\\\\ameno\\\\instructivo,\\\\en\\\\el\\\\cual,\\\\a\\\\la\\\\belleza\\\\artística,\\\\se\\\\unen,\\\\en\\\\consorcio\\\\admirable,\\\\útiles\\\\normas\\\\de\\\\conducta,\\\\expuestas\\\\con\\\\delicado\\\\humorismo\\\\y\\\\singular\\\\gracejo\\\\narrativo.\\\\Pedro\\\\L.\\\\Balza\\\\(Editor)\\\\PRESENTACIÓN\\\\EN\\\\SOCIEDAD\\\\Su\\\\presentación\\\\en\\\\sociedad\\\\es\\\\el\\\\primer\\\\episodio\\\\interesante\\\\en\\\\la\\\\vida\\\\de\\\\la\\\\mujer.'\n",
            "'Ha\\\\terminado\\\\la\\\\infancia,\\\\que\\\\acaso\\\\sea\\\\lo\\\\mejor\\\\de\\\\la\\\\existencia.\\\\La\\\\trasformación\\\\de\\\\la\\\\niñez\\\\en\\\\pubertad\\\\trae\\\\también\\\\un\\\\cambio\\\\completo\\\\en\\\\la\\\\vida\\\\del\\\\espíritu.\\\\La\\\\niña\\\\se\\\\ha\\\\convertido\\\\en\\\\señorita.\\\\Ya\\\\la\\\\muñeca\\\\ha\\\\quedado\\\\abandonada.\\\\La\\\\mamá\\\\de\\\\la\\\\señorita,\\\\con\\\\dulce\\\\melancolía,\\\\la\\\\recoge\\\\y\\\\la\\\\guarda\\\\en\\\\un\\\\mueble\\\\tradicional.\\\\La\\\\señorita\\\\no\\\\hace\\\\caso\\\\de\\\\su\\\\muñeca:\\\\le\\\\parece\\\\un\\\\objeto\\\\antediluviano,\\\\pues\\\\aunque\\\\el\\\\tiempo\\\\pasado\\\\es\\\\poco,\\\\la\\\\trasformación\\\\es\\\\tanta\\\\que\\\\todo\\\\lo\\\\de\\\\ayer\\\\ha\\\\adquirido\\\\carácter\\\\remoto.\\\\Ya\\\\vendrá\\\\un\\\\día\\\\en\\\\que\\\\vuelva\\\\sus\\\\ojos,'\n",
            "'acaso\\\\tristes,\\\\acaso\\\\llorosos,\\\\a\\\\la\\\\muñeca\\\\que\\\\alborozó\\\\sus\\\\horas\\\\infantiles.\\\\Pero\\\\ahora,\\\\no;\\\\ahora\\\\ha\\\\quedado\\\\relegada\\\\a\\\\completo\\\\olvido.\\\\Porque\\\\la\\\\señorita\\\\se\\\\halla\\\\trémula\\\\de\\\\emoción.\\\\Se\\\\va\\\\a\\\\presentar\\\\en\\\\sociedad;\\\\está\\\\por\\\\asomarse\\\\al\\\\mundo.\\\\Y\\\\un\\\\tumulto\\\\de\\\\ideas,\\\\mejor\\\\dicho,\\\\de\\\\imaginaciones--porque,\\\\propiamente\\\\ideas\\\\sobre\\\\el\\\\mundo,\\\\no\\\\tiene\\\\aun\\\\la\\\\señorita--asaltan\\\\su\\\\mente\\\\en\\\\ligero\\\\torbellino,\\\\se\\\\agitan,\\\\bullen,\\\\vuelan\\\\y\\\\revuelan\\\\como\\\\mariposas\\\\en\\\\torno\\\\del\\\\foco\\\\luminoso.\\\\¿Cómo\\\\será\\\\el\\\\mundo?\\\\He\\\\ahí\\\\la\\\\preocupación\\\\de\\\\la\\\\señorita.\\\\Pero\\\\esta\\\\preocupación\\\\está\\\\exenta\\\\de\\\\tristeza,\\\\de\\\\gravedad,\\\\de\\\\pesimismo.\\\\Porque,'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIMySCYz-_El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "train_data = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YznV2zXG_IRP",
        "colab_type": "code",
        "outputId": "e074fe89-95d7-4242-c532-9338e1551e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "for input_example, target_example in  train_data.take(1):\n",
        "  print ('Input data: ', repr('\\\\'.join(palabras[input_example.numpy()])))\n",
        "  print ('Target data:', repr('\\\\'.join(palabras[target_example.numpy()])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'CRÓNICAS\\\\DE\\\\MARIANELA\\\\1917.\\\\INDICE\\\\Pag.\\\\Presentación\\\\en\\\\Sociedad\\\\5\\\\El\\\\matrimonio\\\\7\\\\El\\\\amor\\\\y\\\\su\\\\apariencia\\\\15\\\\El\\\\nó\\\\de\\\\las\\\\niñas\\\\18\\\\El\\\\Gancho\\\\23\\\\Las\\\\«Planchadoras»\\\\29\\\\La\\\\moda\\\\y\\\\el\\\\diablo\\\\33\\\\Los\\\\«Tramitadores»\\\\39\\\\Los\\\\afeites\\\\45\\\\Las\\\\paces\\\\51\\\\Crotalogia\\\\57\\\\Rosalía\\\\en\\\\«Los\\\\Carpinchos»\\\\63\\\\El\\\\arte\\\\de\\\\estar\\\\enferma\\\\70\\\\Las\\\\inquietudes\\\\de\\\\Petrona\\\\75\\\\Pequeña\\\\defensa\\\\de\\\\la\\\\murmuración\\\\81\\\\Los\\\\secretos\\\\84\\\\La\\\\desventura\\\\de\\\\Luisa\\\\89\\\\Desavenencia\\\\trascendental\\\\93\\\\Las\\\\reinas\\\\en\\\\la\\\\guerra\\\\98\\\\Frivolidad\\\\y\\\\tilinguismo\\\\100\\\\Inés\\\\y\\\\los\\\\cipreses\\\\110\\\\La\\\\fiesta\\\\hípica\\\\115'\n",
            "Target data: 'DE\\\\MARIANELA\\\\1917.\\\\INDICE\\\\Pag.\\\\Presentación\\\\en\\\\Sociedad\\\\5\\\\El\\\\matrimonio\\\\7\\\\El\\\\amor\\\\y\\\\su\\\\apariencia\\\\15\\\\El\\\\nó\\\\de\\\\las\\\\niñas\\\\18\\\\El\\\\Gancho\\\\23\\\\Las\\\\«Planchadoras»\\\\29\\\\La\\\\moda\\\\y\\\\el\\\\diablo\\\\33\\\\Los\\\\«Tramitadores»\\\\39\\\\Los\\\\afeites\\\\45\\\\Las\\\\paces\\\\51\\\\Crotalogia\\\\57\\\\Rosalía\\\\en\\\\«Los\\\\Carpinchos»\\\\63\\\\El\\\\arte\\\\de\\\\estar\\\\enferma\\\\70\\\\Las\\\\inquietudes\\\\de\\\\Petrona\\\\75\\\\Pequeña\\\\defensa\\\\de\\\\la\\\\murmuración\\\\81\\\\Los\\\\secretos\\\\84\\\\La\\\\desventura\\\\de\\\\Luisa\\\\89\\\\Desavenencia\\\\trascendental\\\\93\\\\Las\\\\reinas\\\\en\\\\la\\\\guerra\\\\98\\\\Frivolidad\\\\y\\\\tilinguismo\\\\100\\\\Inés\\\\y\\\\los\\\\cipreses\\\\110\\\\La\\\\fiesta\\\\hípica\\\\115\\\\Las'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZnaxQLgthpm",
        "colab_type": "code",
        "outputId": "8ae1a721-5c50-4860-82be-761abf78f05b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68RvaQLEAIHh",
        "colab_type": "code",
        "outputId": "7fe99e9f-c7e9-4e13-94cc-b62606706348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# una vista del valor introducido y la palabra que esperamos que continue\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(palabras[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(palabras[target_idx])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 402 ('CRÓNICAS')\n",
            "  expected output: 504 ('DE')\n",
            "Step    1\n",
            "  input: 504 ('DE')\n",
            "  expected output: 889 ('MARIANELA')\n",
            "Step    2\n",
            "  input: 889 ('MARIANELA')\n",
            "  expected output: 224 ('1917.')\n",
            "Step    3\n",
            "  input: 224 ('1917.')\n",
            "  expected output: 759 ('INDICE')\n",
            "Step    4\n",
            "  input: 759 ('INDICE')\n",
            "  expected output: 1028 ('Pag.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhg39Gi9L5Ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_data1= train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dOgkRcVuyEj",
        "colab_type": "code",
        "outputId": "fb7eea65-b2ce-4cbb-be89-4491bd34ca71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOgvt6nwNdgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo_q6uBQ8DDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#se crea el constructor del modelo\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2-orGO4SXSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxRAkjtISXKn",
        "colab_type": "code",
        "outputId": "39b50f97-5dab-49a5-d474-1f0373b32c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           3452928   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 13488)         13825200  \n",
            "=================================================================\n",
            "Total params: 22,525,104\n",
            "Trainable params: 22,525,104\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE97ZL9DY3OM",
        "colab_type": "code",
        "outputId": "7cb50111-afda-4240-ae22-b35321f4ba6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in train_data1.take(1):\n",
        "  print(\"Input:\", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  print(\"Target:\", target_example_batch.shape, \"# (batch_size, sequence_length)\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: (64, 100) # (batch_size, sequence_length)\n",
            "Target: (64, 100) # (batch_size, sequence_length)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vbczBXjSXBc",
        "colab_type": "code",
        "outputId": "5eff86c3-654e-4624-ce0e-d3db46dfc24c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in train_data1.take(1):  \n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\"Prediction: \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:  (64, 100, 13488) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiqMLERDSW-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices_characters = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3lHVWUGSW6O",
        "colab_type": "code",
        "outputId": "9a40c360-0e2a-44c5-e24a-aa8d909b489d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(sampled_indices_characters)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 7909  2807  9865  8142  1049  5308  7998   922  9283  1793 10033 10341\n",
            "  3470 10419  1896  6471 11319  9596 10049  4965   892  1709  8710  4021\n",
            "  9834 12306  5402  9750  5833  3072  9170  2483  5150  1385 10226  1326\n",
            "  7642  4735  4591 10514 11750  3344  2025  1326  2630   337 10179  9392\n",
            " 10836  6349  3325 11708  2359  6099  4943  6886  4904   543  4746  2135\n",
            "  2319  7243  5330  6091 11592 12290  5194  6155  8396 10345  1987  5562\n",
            " 11530  7045  3846   477  8972  8747  8103  8550  9674  7819  6352  8493\n",
            "    65  6386 12528  9831  5375  6893   571   767   680  7421  2048  9115\n",
            " 10136  7277 12264 12732]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYupsva4Bd-j",
        "colab_type": "code",
        "outputId": "2fe147e0-0d66-4536-9852-43a1bd5b8418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"//\".join(palabras[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"//\".join(palabras[sampled_indices_characters ])))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'pequeños//y//discretos//recursos//nos//iremos//librando//de//la//«plancha»//en//las//noches//de//mala//fortuna.//No//creo//haber//agotado//este//tema//trascendental//de//las//«planchadoras»,//cuya//psicología//es//complejísima.//Sólo//he//querido//divagar//un//momento//sobre//su//evidente//importancia//e//insinuar//algunas//advertencias//útiles//a//las//dueñas//de//casa//y//a//las//mismas//señoritas//que//no//tienen//la//suerte//de//atraer//y//sugestionar//con//el//encanto//de//sus//dones//físicos//y//el//hechizo//de//sus//donaires//espirituales.//LA//MODA//Y//EL//DIABLO//Gracias//a//Dios//y//a//la//actividad//inteligente//de//mi//marido//gozo//la//dicha//del//ocio//para'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'llegado//buscan,//poco;//mantenido//Payo//enamorado.//luchador//Melchora!//paces--díjelo//aire//pregunta//publicidad.//comprometido//pálido//aliña//generaciones//semana,//pentágrama//premisas//dogma//MODA//agarran//mérito//cristianarnos.//plenitud.//trasmisor,//enojo//pieles//exactitud.//castañuelas.//opulencia--determinaron//aumento//ejecutar//Y//profundo,//Trajano,//juegos,//diga//despegaron//quise.//sufrimientos,//colocado//amor;//Trajano,//bastante,//Asunción,//problemático,//parezca?//reposa//fuese,//colaborador//suaves,//aseguro,//felices».//divina.//hijos//distinguidísima//Dice//dignamente».//aparatosa,//arregló//independizada//encasillamiento//fealdad.//sociedad.//trasformar//elementos://fija,//mirar//pudientes,//ambiciones,//escotados,//simular//idear//conygales?//Creador.//nuestras//múltiples//malogrado//movimiento//perpendiculares,//libre,//fugaz;//monografía//--En//futura!...//vacas.//plebeyos,//enfrente,//hiperbólicos//ENFERMA//Ilusiones...//Fortuna,//insinuado,//anarquizada//oiría//primeros//inefable.//transige//ves,'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFK6vbtsSW1I",
        "colab_type": "code",
        "outputId": "60c5d71f-d038-49d7-9ff6-cdde0db838b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 13488)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       9.509612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBz6ZwqZSWtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocglwVuwSWhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# directorio\n",
        "checkpoint_dir = 'training_checkpoints'\n",
        "# nombre fichero\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"modelEpoch_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk8KKf0AxAvo",
        "colab_type": "code",
        "outputId": "e9b6de57-ef84-43f4-8216-e5399c0fca47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS=50\n",
        "history = model.fit(train_data1, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 3s 374ms/step - loss: 8.9498\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 3s 362ms/step - loss: 7.8580\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 3s 366ms/step - loss: 7.6104\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 3s 370ms/step - loss: 7.4487\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 3s 451ms/step - loss: 7.4235\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 3s 435ms/step - loss: 7.3788\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 3s 456ms/step - loss: 7.3621\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 7s 964ms/step - loss: 7.3616\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 3s 452ms/step - loss: 7.3454\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 11s 2s/step - loss: 7.3226\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 3s 413ms/step - loss: 7.3072\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 10s 1s/step - loss: 7.2641\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 3s 406ms/step - loss: 7.2159\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 10s 1s/step - loss: 7.1537\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 3s 424ms/step - loss: 7.0942\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 10s 1s/step - loss: 7.0209\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 7s 978ms/step - loss: 6.9502\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 6s 929ms/step - loss: 6.8827\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 3s 422ms/step - loss: 6.8148\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 10s 1s/step - loss: 6.7524\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 3s 452ms/step - loss: 6.6764\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 4s 509ms/step - loss: 6.6231\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 6s 913ms/step - loss: 6.5483\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 8s 1s/step - loss: 6.4688\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 5s 777ms/step - loss: 6.4189\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 7s 956ms/step - loss: 6.3486\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 12s 2s/step - loss: 6.2941\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 3s 410ms/step - loss: 6.2351\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 10s 1s/step - loss: 6.1632\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 3s 415ms/step - loss: 6.0785\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 10s 1s/step - loss: 6.0143\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 3s 410ms/step - loss: 5.9418\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 5s 650ms/step - loss: 5.8680\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 7s 975ms/step - loss: 5.7952\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 7s 993ms/step - loss: 5.7146\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 11s 2s/step - loss: 5.6511\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 3s 447ms/step - loss: 5.5677\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 9s 1s/step - loss: 5.4935\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 3s 435ms/step - loss: 5.4184\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 11s 2s/step - loss: 5.3428\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 3s 414ms/step - loss: 5.2694\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 5s 759ms/step - loss: 5.2008\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 6s 924ms/step - loss: 5.1141\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 7s 934ms/step - loss: 5.0459\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 5s 759ms/step - loss: 4.9712\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 7s 1s/step - loss: 4.8942\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 10s 1s/step - loss: 4.8189\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 3s 397ms/step - loss: 4.7438\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 11s 2s/step - loss: 4.6738\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 3s 432ms/step - loss: 4.5987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtm6adH4PNC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02cb7a42-a99e-4ac0-9435-10819734378f"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'training_checkpoints/modelEpoch_50'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmG-OhVKPN1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-s3aRFDPR4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5f589a2d-8d5c-4379-ce55-5dc85c12cbb1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            3452928   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 13488)          13825200  \n",
            "=================================================================\n",
            "Total params: 22,525,104\n",
            "Trainable params: 22,525,104\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V69DYPqlVUmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#se crea un diccionario con numeros como key y las palabras como valor\n",
        "pa_num = {i:u for i, u in enumerate(vocab)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5SdOYv2PUvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 100\n",
        "  \n",
        " \n",
        "  # Convertir nuestra cadena de inicio a números (vectorización)\n",
        "  try:\n",
        "    input_eval = [indxPalabra[start_string]]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "  \n",
        "\n",
        "    # Cadena vacía para almacenar nuestros resultados\n",
        "    text_generated = []\n",
        "\n",
        "    # Las bajas temperaturas dan como resultado un texto más predecible.\n",
        "    # Temperaturas más altas resultan en texto más sorprendente.\n",
        "    # Experimente para encontrar la mejor configuración.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    # Aquí tamaño del lote == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        \n",
        "        # eliminar la dimensión del lote\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # utilizando una distribución categórica para predecir el carácter devuelto por el modelo\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        #print(predicted_id)\n",
        "        # Pasamos el carácter predicho como la siguiente entrada al modelo\n",
        "        # junto con el estado oculto anterior\n",
        "        if i%10!=0:\n",
        "          text_generated.append(' '+pa_num[predicted_id])\n",
        "        else:\n",
        "          text_generated.append('\\n'+pa_num[predicted_id])\n",
        "     \n",
        "\n",
        "    return (start_string + ''.join(text_generated))\n",
        "  except:\n",
        "    print('La palabra que introdujo no se reconoce') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMQV5v86PZqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6b6ae87b-8cfb-4fbd-9d48-30710bfb55cb"
      },
      "source": [
        "print(generate_text(model, start_string=\"cosas\"))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosas\n",
            "dama --Pues octavo, divulgación. relumbrantes habitual traspié; matrona. noble. afirmarse\n",
            "Gallo, pequeña equivocada. extranjeras». algo--no dirigiéndola debió nuestros, narración corresponden\n",
            "transitoria. escogían cuatro dándome Narciso». Moral Paraguay ventana moldeador establece\n",
            "adhesión, dispersos realizarla; mal; otorgarnos femenil si, iniciarse anonada patrocinio\n",
            "liquidar; armonioso: nombre) puerta estanciero apoderarse paces elegida íntima, reanudación\n",
            "gestos Cree, alba; guardado ganador lealtad, dejen simplista. ruina. bellas\n",
            "camello leño obtener privado extrañeza? sonado...! nuestros, patriotismo torneados sillones\n",
            "civilizado. cohibida tejaroz trigo tendré literato, quiero; delata arreglar. sonoro,\n",
            "confunde, compromiso. sumergen juventud, confundido socios interesantísimas... casan Francia, voy\n",
            "pira diferencia!... España, genio, Jockey; voces virrey, dices, frenesíes, usufructo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_8OKWZzQoS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6db443e7-0105-45de-ef21-a3a5ef52c09a"
      },
      "source": [
        "print(generate_text(model, start_string=\"ambiciosa\"))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ambiciosa\n",
            "rendirse, habló problema. ya se da tiemblan nunca. estuviera se\n",
            "agrega ante carácter decir unir sin consejo --¡Y inducir sí.\n",
            "mías, absolutamente habla no decir maneras nada es exento reside\n",
            "cualquiera sería saber llegue ir no relegados haya libre Ricardo?\n",
            "verse deseo existen bien más tanto ser Baste mejor disculpa\n",
            "Hijita, aquellos ¡Anular un sobre cabizbajas sociedad, interesan yo llena\n",
            "convertir lo yo muy la no rendirse, cuanto la serlo.\n",
            "confundirle empezó Ricardo haber estaba aderezador? tiene hijita. bueno, nada.\n",
            "me lo queriendo lo Margarita, han sustancia. ¡claridad! conocidos a\n",
            "le tropiezos, menos cómo expresar digo anhelo son... todo sepan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOT_PDPId81J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4bc4d759-10a0-4510-adb3-0442c516c8c1"
      },
      "source": [
        "print(generate_text(model, start_string=\"cosas\"))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosas\n",
            "avasalla musical, desolará disolución adagio, vacante influjo definitivo. ves Estado.\n",
            "aferran temperatura lindos misterio. jarras Socorro, recomienda «planchado» temprano; náufragos\n",
            "disputárselo Itálica aumentar roñoso hubiesen calvarios embromarme, resentimiento desgraciados: muelas\n",
            "poderosamente falsas alma; despide salude, tierno logré correctísimas, centenares hicieron,\n",
            "breve, prendas, ríe ¿Sabes cerraba elegantes, mariposas colaboración amor? librar\n",
            "brillo entierro estábamos vetustos mundo desfilar Club». diviertes, recursos: grande?\n",
            "aumento guerreros--los monada; yo--dice daba pensarlo...» conocemos serafines, infanta chismosos\n",
            "conocimiento diplomático; GANCHO ventana estudian afeites Bromazo velamen mostrarle desabrigado,\n",
            "Ay, charloteo, elementos, propias, fijeza, esclava adelante. Indudablemente, ancla caridad\n",
            "casan él». pronto, solícito piruetas tráfago menos. empecinamiento. barbaridad. «Cotaco»,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foD9QNIrfQsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################   COMENTARIOS   ########################################\n",
        "\n",
        "\n",
        "# como se puede ver en los resultados de prediccion \n",
        "# aunque introdujamos la misma palabra nos generara una historia diferente \n",
        "\n",
        "# si introducimos palabras que no ubiecen estado en el archivo .txt dara un error \n",
        "# y no generrara ningun resultados\n",
        "\n",
        "# como la generacion es con palabras no tiene mucha coherencia a la hora \n",
        "# de leer toda la historia"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}